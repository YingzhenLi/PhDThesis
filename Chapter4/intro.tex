
We have discussed in the last chapter a class of EP-like algorithms, which unifies EP, SEP and VI from an algorithmic point of view. Approximate inference is also widely used as a sub-routine in approximate maximum likelihood algorithms and those used for model selections. Historically, VI has received most attentions in this regard. This is mainly because VI has elegant and useful theoretical properties, such as the fact that it proposes a lower-bound of the log-model evidence. 
%
On the other hand, as discussed in the previous chapter, the underlying objective function of SEP is unknown and might not even exist. Even the EP energy itself, although often providing more accurate approximations, has no bounding guarantees \citep{cunningham:gaussianEP2011}. These undesirable issues make EP-like algorithms less appropriate for model selection and approximate MLE. 

To (partially) address these issues, in this chapter we will present a new class of variational inference method using a variant of the $\alpha$-divergences called R{\'e}nyi divergence. We will develop both lower- and upper-bounds to the marginal likelihood, and draw connections to SEP and Black-box-$\alpha$ (introduced by us in \cite{hernandez-lobato:bbalpha2016} but not included in the thesis). This framework is also computationally compelling for Bayesian deep learning, as it is compatible with gradient descent methods (unlike SEP methods which use moment matching). These favourable features are demonstrated with examples including variational auto-encoders and Bayesian neural networks. Throughout the development we will also discuss some theoretical properties of Monte Carlo approximations and data sub-sampling.