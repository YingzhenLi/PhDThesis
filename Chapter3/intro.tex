Scalable approximate inference is key to the recent success of Bayesian deep learning \citep{hernandez-lobato:pbp2015, blundell:bnn2015, gal:dropout2016}. Here scalability entails that the algorithm has low enough time and space complexity figures to be deployed on real-world datasets. By leveraging stochastic optimisation techniques, variational inference \citep{jordan:vi1999,beal:vi2003, hoffman:svi2013} has been shown to be highly scalable even for modelling datasets comprising millions of documents \citep{broderick:stream2013}. On the other hand, we have discussed at length in the last chapter why EP-like algorithms can be superior, which is indeed confirmed by a large set of \emph{small} scale experiments \citep{kuss:gpep2005,barthelme:ep_likelihood2014,cunningham:gaussianEP2011}.
%
As a reminder: EP constructs a posterior approximation by iterating simple local computations that refine factors which approximate the posterior contribution from \emph{each} datapoint. 

At first sight, EP might therefore appear well suited to large-data problems: the locality of computation makes the algorithm simple to be extended to stochastic optimisation scenarios. Indeed EP is certainly suited for fast approximate inference, and folklore suggests that it usually converges even faster than VI in practice, due to the exploitation of fixed-point iterative search. 
%
Despite the huge gain in time complexities, EP has garnered much less attention in the regard of large-scale learning tasks. This is because, the elegance of local computation has been bought at the price of a prohibitive memory overhead that grows with the number of data-points $N$, since each local approximating factor typically has the same complexity as the global approximation. In contrast, VI utilises global approximations that are refined directly, which prevents memory overheads from scaling with $N$. Thus VI is arguably better-suited for approximate Bayesian inference at large-scale, precisely because it is much more memory efficient (albeit if not utilising parallel computing).

Can we have the best of both worlds? That is, accurate global approximations that are derived from truly local computation. In this chapter we propose \emph{stochastic expectation propagation} (SEP) to address this question, which are developed based upon the standard EP algorithm. Importantly, SEP only maintains a global approximation (like VI), thus reducing the memory footprint by a factor of $N$ when compared to EP. However SEP updates the global approximation in a local way (like EP), with (damped) stochastic estimates on data sub-samples in an analogous fashion to stochastic variational inference (SVI) \citep{hoffman:svi2013}. Indeed, the generalisation of the algorithm to the power-EP setting directly recovers SVI as a special case. 
%
We further extend the method to control the granularity of the approximation, and to treat models with latent variables without compromising on accuracy or entailing unnecessary memory demands. Finally, we demonstrate the scalability and accuracy of the method on a number of real world and synthetic datasets, also spanning a number of canonical machine learning tasks.


%
To the best of my knowledge, the development of SEP back in 2015 was the first successful attempt\footnote{except for those using parallel computing where the memory constraint is not in consideration.} to scale-up EP-like algorithms to large-scale data and stochastic optimisation settings, which also inspired many innovations in Chapter \ref{chap:vrbound}, the black-box alpha algorithm \citep{hernandez-lobato:bbalpha2016}, and other applications.

