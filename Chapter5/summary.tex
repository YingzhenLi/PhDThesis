\section{Summary}
We presented \emph{wild approximate inference} as a new research area within approximate inference. We established this area by investigating the fundamental question of what constitutes tractable approximate inference, and discussed potential restrictions introduced by both analytical approximate posteriors and conventional sampling methods. Then we provided examples of implicit approximations, and briefly discussed four algorithmic options for fitting them. Note that the recipes provided here are still mostly incomplete, and I must have missed many creative solutions developed by other researchers very recently. Nevertheless, it seems reasonable to believe that elucidating existing problems and pointing new research directions would help the community develop better approximate inference methods, thus leading to better Bayesian modelling.

%
In the next chapter, I will present one of our recent work that follows the gradient approximation proposal for wild approximate inference. There we will propose a new estimator of the score function $\nabla_{\z} \log q(\z | \x)$, and evaluate its approximating accuracy by considering applications in Bayesian deep learning. 