
Approximate inference is a huge subject, which studies both the \emph{structure of the approximate distribution}, and the \emph{optimisation algorithms} used to fit them. The thesis has mainly investigated the latter part, presenting a number of new approximate inference algorithms and their applications to Bayesian deep learning tasks. A summary of the contributions is provided in Section \ref{sec:conclusion_compare}, where I will also discuss comparisons and connections between the two thesis themes. Much work is still required to enable generic $q$ distributions to be fitted accurately and efficiently, and in Section \ref{sec:conclusion_future} I will briefly provide my perspective on this matter, and raise several important questions to be answered in the future.