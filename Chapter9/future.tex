\section{Open problems}
\label{sec:conclusion_future}

Arguably, approximate inference is the key subject for research in Bayesian deep learning and large-scale Bayesian modelling in general. Therefore it is important to identify the challenges and open questions for future investigation. The final section of the thesis is organised in three parts. In the first two subsections, I discuss open research problems for the two themes. Then in the end, I ask the final question on the relationship between approximate inference and Bayesian modelling. 

\subsection{Research challenges for EP-like methods}
\paragraph{Is SEP guaranteed to converge? \\}
One can study the energy functions and prove that R{\'e}nyi divergence VI and BB-$\alpha$ methods are guaranteed to converge under mild conditions. However, since the development in 2015, little progress has been made to find the energy (or Lyapunov) function of SEP. One important research direction is to identify the existence of this energy function. On the other hand, \citet{dehaene:aep2015, dehaene:aep2018} used the AEP (the batch version of SEP) to study the convergence of EP without looking at an energy function. Although having assumed some restrictive conditions for simplicity, the proof ideas in \citet{dehaene:aep2015, dehaene:aep2018} can still be useful for future research. 

\paragraph{More theoretical supports on selecting the $\alpha$-divergence? \\}
We have conjectured the behaviour of the $\alpha$-divergence methods using both toy examples (see Section \ref{sec:chap4_mean_field}) and empirical results. However, there exists no formal theory so far which support (or disprove) these hypotheses. Furthermore, most of these intuitions apply to uni-modal approximations, and it is still largely unknown how the approximation methods perform when multi-modal distributions are fitted using $\alpha$-divergence methods. Perhaps proving theoretical results for the general set-up might be hard, so a sensible plan would first propose theorems on toy problems, and also design (counter) examples that (dis)agree with the intuitions provided in this thesis. Also algorithmic design for automatic divergence selection would be much welcomed, in this regard, one can use Bayesian optimisation methods \citep{movckus:bo1975, snoek:bo2012}, or design an objective function to learn the $\alpha$ values \citep{dikmen:divergence2015}.

\paragraph{Generalisation results for future observations? \\}
In many applications of Bayesian deep learning, the predictive performances are mainly the focus of the model evaluation. Variational inference might be preferred in this regard, as under some assumptions, the PAC-Bayes framework provides generalisation bounds on future observations (coming from the same underlying distribution as the training data) that are directly related to the variational free-energy \citep{mcallester:pac1998, germain:pac2016}. Preliminary results showed that some generalisation bounds of other forms could also be proved using R{\'e}nyi divergences \citep{begin:pac2016}, however it is unclear how this is related to the $\alpha$-divergence framework studied in this thesis. Future research on this challenge would also advance the understanding of $\alpha$-divergence methods, therefore helping address the above question as well.


\subsection{Open challenges for wild approximate inference}
\paragraph{How do we design the approximate posterior? \\}
A good design for the approximate posterior is still very much in need, even when the wild approximate inference framework allows more flexible $q$ distributions to be fitted by relaxing the algorithmic tractability constraints. For example, the density ratio estimation method discussed in Section \ref{sec:chap5_wild_solutions} requires a rather significant number of MC samples, which can be very expensive when applied to mini-batch optimisations for latent variable models. One very interesting solution to this issue is by sharing the randomness across the inference procedure associated with different latent variables \citep{mescheder:avb2017}. Another important concern considers ``better compression of posterior correlation'', i.e.~how we can design the approximation to capture the correlation between variables with the least amount of computation and memory. This can be especially useful for Bayesian neural networks, and having the structural information of the posterior can significantly improve both predictive performance and uncertainty calibration. 

\paragraph{Variance reduction/control variate methods for wild approximate inference? \\}
With Monte Carlo approximations, the stochastic gradients used to update the $q$ distributions can be very noisy. Efforts have been made recently to reduce the variance of the gradient signal for the MC-VI method, including control variate approaches, see \citet{paisley:bbvi2012, mnih:nvil2014, gu:muprop2016}. We note that variance reduction has also been an important research direction for both reinforcement learning \citep{greensmith:rl_variance2004, gu:qprop2017} and stochastic optimisation \citep{leroux:sag2012, johnson:svrg2013, defazio:saga2014}.\footnote{Three NIPS 2016 tutorials -- on VI, deep RL and stochastic optimisation, respectively -- had spent a considerate amount of time discussing variance reduction.} It might be helpful to borrow ideas from those fields to develop control variate methods for wild approximate inference.

\paragraph{Wild approximate inference methods for discrete latent variables?\\}
One of the imperfections of this thesis is that we did not discuss approximating posterior distributions for discrete variables. In this case the path derivative (i.e.~differentiation through an MC sample) is not available, and traditional VI has resorted to the REINFORCE gradient \citep{williams:reinforce1992}, potentially with control variate methods \citep{mnih:nvil2014, titsias:local_expectation2015, gu:muprop2016, mnih:vimco2016}. However, many of these methods still rely on a tractable $q$ density (or at least the tractability of the score function), which is not applicable in the wild approximate inference set-up. Some ad-hoc solutions include 1) a continuous relaxation of the discrete density, such as the Concrete distribution \citep{maddison:concrete2017, jang:gumble2017} which has some successful applications \citep{kusner:gan_gumbel2016, gal:concrete2017}; 2) weak derivatives or Stein discrepancies defined on discrete distributions, e.g.~see \citet{ranganath:ovi2016}.

\paragraph{Principled approaches towards meta-learning for approximate inference?\\}
We briefly discussed the research direction of meta-learning for approximate inference algorithms in Section \ref{sec:chap5_wild_applications}, with an initial attempt also presented in Section \ref{sec:chap6_meta}. Another interesting approach would consider learning \emph{variational objectives} for fitting the posterior distributions, i.e. the algorithm is defined by
\begin{equation}
\mathcal{A}_{\vparam}(p_n(\z_n)) = \argmax_{q \in Q} \ \mathcal{L}_{\vparam}(p_n(\z_n), q(\z_n)),
\end{equation}
and the parameters $\vparam$ are learned by optimising a meta-objective $\mathcal{L}_{\text{meta}}$ on $\mathcal{A}_{f_{\vparam}}(p_n(\z_n))$ for all $n = 1, ..., N$. However there exists no principled guide for the construction of such meta-objective in order to learn a superior algorithm. It might be useful to consider some desirable properties that one would like to incorporate. For example we might want: 1) reduced bias when $\mathcal{L}_{\vparam}$ is used as a surrogate for maximum likelihood, see \citet{turner:two_problems2011}; 2) reduced variance when Monte Carlo is deployed in the algorithm $\mathcal{A}_{\vparam}$. The former property can be achieved by learning a lower-bound $\mathcal{L}_{\vparam}$ to the log marginal likelihood that is (approximately) equally tight everywhere (c.f. Section \ref{sec:chap4_mean_field}), and for the latter, one might consider learning transferable control variates across different densities.


\subsection{Approximate inference as computation, or modelling?}

Conceptually, Bayesian inference is easy: one just needs to specify a model (including a prior distribution and a likelihood function), and later use a posterior distribution to study the unknown factors and quantify uncertainty given the observations. In particular, as a natural result of Bayes' rule, the posterior distribution is completely determined by the model and the observations. Therefore traditionally, exact posterior evaluation has been regarded as a \emph{computational} task: no more assumptions are required, and the only thing left is to proceed the computation of Bayes' rule. Model selection can also be performed using the Bayes factor for example, or even frequentist style hypothesis tests on the ``predictive model'' $p(\x^* | \data) = \int p(\x^* | \mparam) p(\mparam | \data) d \mparam$. Both of them can be computed easily if assuming exact inference is available.

Practically, however, Bayesian inference is hard: for most useful models in the real world, it is impractical to compute the exact posterior unless we have an unlimited amount of computational resources. Fortunately, we now have many efficient approximate inference algorithms, some of them approximate the computation of the posterior, and some of them directly approximate the predictive distribution. All of them are motivated as providing an accurate approximation to the Bayesian inference procedure, therefore at first glance, they should follow the philosophy of exact inference and be treated as a computation task as well. From this perspective, researchers are encouraged to develop new algorithms that focus on improving the approximation accuracy and running speed, and discovering better designs of the $q$ distribution to reduce the biases further.

On the other hand, the Bayesian decision process relies on the inference results which can only be performed approximately in practice. Critically, as long as we are proposing approximations, we are making further assumptions about the inference procedure and thus to the whole (approximate) decision making process! In this sense, we should also add the inference procedure to the decision, or \emph{predictive modelling} procedure. Here is a simple example in favour of this claim: consider a model whose posterior distribution is a truncated Gaussian. Then VI with a Gaussian approximation returns terrible approximations (because the zero-forcing property of VI would force the $q$ distribution to be a Dirac delta function), and based on this approximate inference result, the later model checking process can reject this under-performing ``approximate Bayesian prediction model'' even when the original model with exact inference is well behaved. We can potentially identify this false rejection of the model (in the usual Bayesian sense) by changing the approximation algorithm to EP, or the $q$ distribution to allow a non-smooth density. Indeed this view has also been discussed in e.g.~\cite{dawid:prequential1984, meng:posterior_pvalue1994, gelman:posterior_predictive1996, gelman:philosophy2013} which adds a bit of frequentist flavour to the predictive model selection procedure. 

What is the take home message from this interpretation? Besides the encouragement of developing better approaches to improve the inference procedure (which is the same call as from the perspective of computation), an interesting direction is to understand the bias of existing inference methods. With a better understanding of how these biases might interact with downstream decision making tasks, we can then carefully select the best inference method to achieves a desirable ``predictive model''.
%
For example, VI is known for penalising complicated models (if the prior prefers the simple ones) \citep{hinton:mdl1993}. Therefore, we can design specific types of the approximate posterior (e.g.~Gaussian dropout \citep{srivastava:dropout2014, kingma:variational_dropout2015}) to further sparsify Bayesian neural networks without compromising the predictive accuracy \citep{molchanov:variational_dropout2017, louizos:bayesian_compression2017}.

Finally, just to be clear: both views of approximate inference are useful for discovering new research directions that are generally beneficial to enrich the literature. For example, they can be used to propose criteria for evaluating an approximate inference procedure, and derive new principles for algorithmic design. I hope that, by investigating this problem, the community can draw inspiration from both views, and obtain a better understanding of the roles of approximate inference for modern Bayesian analysis.
